{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import fmin, hp, space_eval, tpe, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope, stochastic\n",
    "from plotly import express as px\n",
    "from plotly import graph_objects as go\n",
    "from plotly import offline as pyo\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIAN_HOME_VALUE = \"median_home_value\"\n",
    "\n",
    "# Load the boston dataset using sklearn's helper function\n",
    "boston_dataset = load_boston()\n",
    "# Convert the data into a Pandas dataframe\n",
    "data = np.concatenate(\n",
    "    [boston_dataset[\"data\"], boston_dataset[\"target\"][..., np.newaxis]],\n",
    "    axis=1,\n",
    ")\n",
    "features, target = boston_dataset[\"feature_names\"], MEDIAN_HOME_VALUE\n",
    "columns = np.concatenate([features, [target]])\n",
    "boston_dataset_df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constant strings that we will use as keys in the “search space” dictionary below. Note that\n",
    "# the convention I use throughout is to represent the characters in the string with a variable that\n",
    "# matches that string except that the characters in the variable are capitalized. This convention\n",
    "# allows us to easily interpret what these variable mean when we encounter them in the code. For\n",
    "# example, we know that the variable `MODEL` contains the string “model”. Following this pattern of\n",
    "# representing strings with variables allows me to avoid typos when repeatedly using the same string\n",
    "# throughout the code since typos in variable names will be caught as errors by my linter.\n",
    "GRADIENT_BOOSTING_REGRESSOR = \"gradient_boosting_regressor\"\n",
    "KWARGS = \"kwargs\"\n",
    "LEARNING_RATE = \"learning_rate\"\n",
    "LINEAR_REGRESSION = \"linear_regression\"\n",
    "MAX_DEPTH = \"max_depth\"\n",
    "MODEL = \"model\"\n",
    "MODEL_CHOICE = \"model_choice\"\n",
    "NORMALIZE = \"normalize\"\n",
    "N_ESTIMATORS = \"n_estimators\"\n",
    "RANDOM_FOREST_REGRESSOR = \"random_forest_regressor\"\n",
    "RANDOM_STATE = \"random_state\"\n",
    "\n",
    "# Declare the search space for the random forest regressor model.\n",
    "random_forest_regressor = {\n",
    "    MODEL: RANDOM_FOREST_REGRESSOR,\n",
    "    # I define the model parameters as a separate dictionary so that we can feed the parameters into\n",
    "    # the `__init__` of the model with dictionary unpacking. See the `sample_to_model` function\n",
    "    # that's defined alongside the objective function to see this in action.\n",
    "    KWARGS: {\n",
    "        N_ESTIMATORS: scope.int(\n",
    "            hp.quniform(f\"{RANDOM_FOREST_REGRESSOR}__{N_ESTIMATORS}\", 50, 150, 1)\n",
    "        ),\n",
    "        MAX_DEPTH: scope.int(\n",
    "            hp.quniform(f\"{RANDOM_FOREST_REGRESSOR}__{MAX_DEPTH}\", 2, 12, 1)\n",
    "        ),\n",
    "        RANDOM_STATE: 0,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Declare the search space for the gradient boosting regressor model, following the same structure\n",
    "# as the random forest regressor search space.\n",
    "gradient_boosting_regressor = {\n",
    "    MODEL: GRADIENT_BOOSTING_REGRESSOR,\n",
    "    KWARGS: {\n",
    "        LEARNING_RATE: scope.float(\n",
    "            hp.uniform(\n",
    "                f\"{GRADIENT_BOOSTING_REGRESSOR}__{LEARNING_RATE}\",\n",
    "                0.01,\n",
    "                0.15,\n",
    "            )\n",
    "        ),  # lower learning rate\n",
    "        N_ESTIMATORS: scope.int(\n",
    "            hp.quniform(f\"{GRADIENT_BOOSTING_REGRESSOR}__{N_ESTIMATORS}\", 50, 150, 1)\n",
    "        ),\n",
    "        MAX_DEPTH: scope.int(\n",
    "            hp.quniform(f\"{GRADIENT_BOOSTING_REGRESSOR}__{MAX_DEPTH}\", 2, 12, 1)\n",
    "        ),\n",
    "        RANDOM_STATE: 0,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Combine both model search spaces with a top level \"choice\" between the two models to get the final\n",
    "# search space.\n",
    "space = {\n",
    "    MODEL_CHOICE: hp.choice(\n",
    "        MODEL_CHOICE,\n",
    "        [\n",
    "            random_forest_regressor,\n",
    "            gradient_boosting_regressor,\n",
    "        ],\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few additional variables to represent strings. Note that this code expects that we have\n",
    "# access to all variables that we previously defined in the \"search space\" code snippet.\n",
    "LOSS = \"loss\"\n",
    "STATUS = \"status\"\n",
    "\n",
    "# Mapping from string name to model class deinition object that we'll use to create an initialized\n",
    "# version of a model from a sample generated from the search space by hyperopt.\n",
    "MODELS = {\n",
    "    GRADIENT_BOOSTING_REGRESSOR: GradientBoostingRegressor,\n",
    "    RANDOM_FOREST_REGRESSOR: RandomForestRegressor,\n",
    "}\n",
    "# Create a scoring function that we'll use in our objective\n",
    "mse_scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "\n",
    "# Helper function thta converts from a sample generated by hyperopt to an initialized model. Note\n",
    "# that because we split the model type and model keyword-arguments into separate key-value pairs in\n",
    "# the search space declaration we are able to use dictionary unpacking to create an initialized\n",
    "# version of the model.\n",
    "def sample_to_model(sample):\n",
    "    kwargs = sample[MODEL_CHOICE][KWARGS]\n",
    "    return MODELS[sample[MODEL_CHOICE][MODEL]](**kwargs)\n",
    "\n",
    "\n",
    "# Define the objective function for hyperopt. We'll fix the `dataset`, `features`, and `target`\n",
    "# arguments with `functools.partial` to create that version of this function that we will supply as\n",
    "# an argument to `fmin`\n",
    "def objective(sample, dataset_df, features, target):\n",
    "    model = sample_to_model(sample)\n",
    "    rng = check_random_state(0)\n",
    "    # Handle randomization by shuffling when creating folds. In reality, we probably want a better\n",
    "    # strategy for managing randomization than the fixed `RandomState` instance generated above.\n",
    "    cv = KFold(n_splits=10, random_state=rng, shuffle=True)\n",
    "    # Calculate average mean squared error for each fold. Since `n_splits` is 10, `mse` will is an\n",
    "    # array of size 10 with each element representing the average mean squared error for a fold.\n",
    "    mse = cross_val_score(\n",
    "        model,\n",
    "        dataset_df.loc[:, features],\n",
    "        dataset_df.loc[:, target],\n",
    "        scoring=mse_scorer,\n",
    "        cv=cv,\n",
    "    )\n",
    "    # Return average of mean squared error across all folds.\n",
    "    return {LOSS: np.mean(mse), STATUS: STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we defined our objective function to be generic in terms of the dataset, we need to use\n",
    "# `partial` from the `functools` module to \"fix\" the `dataset_df`, `features`, and `target`\n",
    "# arguments to the values that we want for this example so that we have an objective function that\n",
    "# takes in only one argument as assumed by the `hyperopt` interface.\n",
    "boston_objective = partial(\n",
    "    objective, dataset_df=boston_dataset_df, features=features, target=MEDIAN_HOME_VALUE\n",
    ")\n",
    "# `hyperopt` tracks the results of each iteration in this `Trials` object. We’ll be collecting the\n",
    "# data that we will use for visualization from this object.\n",
    "trials = Trials()\n",
    "rng = check_random_state(0)  # reproducibility!\n",
    "# `fmin` searches for hyperparameters that “minimize” our object, mean squared error and returns the\n",
    "# “best” set of hyperparameters.\n",
    "best = fmin(boston_objective, space, tpe.suggest, 1000, trials=trials, rstate=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint([t for t in trials][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple helper function that allows us to fill in `np.nan` when a particular\n",
    "# hyperparameter is not relevant to a particular trial.\n",
    "def unpack(x):\n",
    "    if x:\n",
    "        return x[0]\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# We'll first turn each trial into a series and then stack those series together as a dataframe.\n",
    "trials_df = pd.DataFrame([pd.Series(t[\"misc\"][\"vals\"]).apply(unpack) for t in trials])\n",
    "# Then we'll add other relevant bits of information to the correct rows and perform a couple of\n",
    "# mappings for convenience\n",
    "trials_df[\"loss\"] = [t[\"result\"][\"loss\"] for t in trials]\n",
    "trials_df[\"trial_number\"] = trials_df.index\n",
    "trials_df[MODEL_CHOICE] = trials_df[MODEL_CHOICE].apply(\n",
    "    lambda x: RANDOM_FOREST_REGRESSOR if x == 0 else GRADIENT_BOOSTING_REGRESSOR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# px is an alias for \"express\" that's created by following the convention of importing \"express\" by\n",
    "# running `from plotly import express as px`\n",
    "fig = px.scatter(trials_df, x=\"trial_number\", y=\"loss\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(trials_df, x=\"trial_number\", y=\"loss\", color=MODEL_CHOICE)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hover_data(fig, df, model_choice):\n",
    "    # Filter to only columns that are relevant to the current model choice. Note that this relies on\n",
    "    # the convention of including the model name in the hyperparameter name when we declare the\n",
    "    # search space.\n",
    "    cols = [col for col in trials_df.columns if model_choice in col]\n",
    "    fig.update_traces(\n",
    "        # This specifies the data that we want to plot for the current model choice.\n",
    "        customdata=trials_df.loc[\n",
    "            trials_df[MODEL_CHOICE] == model_choice, cols + [MODEL_CHOICE]\n",
    "        ],\n",
    "        hovertemplate=\"<br>\".join(\n",
    "            [\n",
    "                f\"{col.split('__')[1]}: %{{customdata[{i}]}}\"\n",
    "                for i, col in enumerate(cols)\n",
    "            ]\n",
    "        )\n",
    "        + \"<extra></extra>\",\n",
    "        # We only apply the hover data for the current model choice.\n",
    "        selector={\"name\": model_choice},\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    trials_df,\n",
    "    x=\"trial_number\",\n",
    "    y=\"loss\",\n",
    "    color=MODEL_CHOICE,\n",
    ")\n",
    "# We call the `add_hover_data` function once for each model type so that we can add different sets\n",
    "# of hyperparameters as hover data for each model type.\n",
    "fig = add_hover_data(fig, trials_df, RANDOM_FOREST_REGRESSOR)\n",
    "fig = add_hover_data(fig, trials_df, GRADIENT_BOOSTING_REGRESSOR)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since max_depth == 3 outperforms other settings, we'll filther to only look at that slice. This\n",
    "# creates a boolean array that we will use to filter down to relevant rows in the `trials_df`\n",
    "# dataframe.\n",
    "max_depth_filter = (trials_df[MODEL_CHOICE] == GRADIENT_BOOSTING_REGRESSOR) & (\n",
    "    trials_df[\"gradient_boosting_regressor__max_depth\"] == 3\n",
    ")\n",
    "\n",
    "# plotly express does not support contour plots so we will use `graph_objects` instead. `go.Contour\n",
    "# automatically interpolates \"z\" values for our loss.\n",
    "fig = go.Figure(\n",
    "    data=go.Contour(\n",
    "        z=trials_df.loc[max_depth_filter, \"loss\"],\n",
    "        x=trials_df.loc[max_depth_filter, \"gradient_boosting_regressor__learning_rate\"],\n",
    "        y=trials_df.loc[max_depth_filter, \"gradient_boosting_regressor__n_estimators\"],\n",
    "        contours=dict(\n",
    "            showlabels=True,  # show labels on contours\n",
    "            labelfont=dict(\n",
    "                size=12,\n",
    "                color=\"white\",\n",
    "            ),  # label font properties\n",
    "        ),\n",
    "        colorbar=dict(\n",
    "            title=\"loss\",\n",
    "            titleside=\"right\",\n",
    "        ),\n",
    "        hovertemplate=\"loss: %{z}<br>learning_rate: %{x}<br>n_estimators: %{y}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"learning_rate\",\n",
    "    yaxis_title=\"n_estimators\",\n",
    "    title={\n",
    "        \"text\": \"learning_rate vs. n_estimators | max_depth == 3\",\n",
    "        \"xanchor\": \"center\",\n",
    "        \"yanchor\": \"top\",\n",
    "        \"x\": 0.5,\n",
    "    },\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
